## 一、使用Optimum-NVIDIA优化
在使用 `Optimum-NVIDIA` 优化推理引擎之前，请确保满足以下条件：
1. **创建虚拟环境**：使用 `conda create` 命令来创建名为 `on` 的虚拟环境，并指定 Python 版本为 3.10.12：
   ```bash
   conda create -n on python=3.10.12
   ```

2. **激活虚拟环境**：
   ```bash
   conda activate on
   ```

3. **安装 CUDA 依赖**：确保系统已经安装 CUDA 12.4，并配置好相关环境。如果使用 `conda` 提供的 CUDA 工具包，可以通过以下命令安装：
   ```bash
   conda install cudatoolkit=12.4
   ```

4. **使用 `pip` 安装 Optimum-NVIDIA**：
    ```bash
    apt-get update && apt-get -y install python3.10 python3-pip openmpi-bin libopenmpi-dev
    python -m pip install --pre --extra-index-url https://pypi.nvidia.com optimum-nvidia
    ```

    *问题：在安装 OpenMPI 时，尽管 mpirun 和 mpicc 存在于 /usr/bin/ 目录中，Python 环境中仍然无法找到 mpi4py。
    ![alt text](image-2.png)
    原因：因为 OpenMPI 的库未正确链接到当前 Python 环境，导致 mpi4py 无法被正确识别和使用。
    解决方案：使用 conda 安装 mpi4py*

5. **在虚拟环境中安装Jupyter和 `ipykernel` 并注册环境为内核**：
   ```bash
   pip install ipykernel
   python -m ipykernel install --user --name on --display-name "Python (on)"
   ```

6. **加载Transformer模型并测试吞吐量**:

    *Optimum-NVIDIA 中的 AutoModelForCausalLM只支持因果语言模型，不支持bert类型的模型。下载模型gemma。
    ![alt text](image.png)
    ![alt text](image-1.png)*
    ```python
    from optimum.nvidia import AutoModelForCausalLM
    from transformers import AutoTokenizer
    import torch
    from time import time

    model = AutoModelForCausalLM.from_pretrained("/root/.cache/huggingface/transformers/gemma/Lucachen/gemma2b",
    use_fp8=True,
    max_prompt_length=1024,
    max_output_length=2048, # Must be at least size of max_prompt_length + max_new_tokens
    max_batch_size=8,)
    tokenizer = AutoTokenizer.from_pretrained("/root/.cache/huggingface/transformers/gemma/Lucachen/gemma2b")
    ```
    模型引擎文件缺失且云服务器不能连接huggingface，出现连接失败。
    ![alt text](image-4.png)
    ![alt text](image-3.png)