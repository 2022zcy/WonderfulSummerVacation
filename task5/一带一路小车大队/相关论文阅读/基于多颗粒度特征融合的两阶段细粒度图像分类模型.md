### 基于多颗粒度特征融合的两阶段细粒度图像分类模型

- **当前大模型检测遇到的问题**

​		现有的大多数方法直接使用网络的最终输出，其中总是包含具有高级语义信息的全局特征。然而，细粒度图像之间的差异反映在	网络前端经常出现的细微局部区域。当背景和物体的纹理相似或背景比例过大时，预测将受到很大影响。并且细粒度视觉分类旨在区	分同一类的不同子类，例如，灯[2,3]、汽车[1,8]、人[4,9]的子类。由于类间变化小，类内变化大，这是一项非常具有挑战性的任	务。

- **能较好处理当前问题的大模型**

  ​	Resnet50和Densenet169，他们主要分为两个步骤：（1）通过使用附加注释或分析响应图来定位判别部分；（2）分别提取这些部分的特征并将这些特征连接起来进行分类。与对象部分定位相比，一些基于自注意力的方法效率更高。通过骨干网络提取的所有特征都具有相同的重要性。对于细粒度图像，判别部分的特征更为重要。这就是自注意力机制发挥作用的地方。这种机制通过对判别特征施加更高的权重，使模型更加关注这部分的信息。但是也存在问题：它们使用单尺度网络输出，这使得人们很容易忽略在预测中发挥关键作用的细微特征。此外，当背景占据比例过大或纹理与物体相似时，最终的预测将受到严重干扰。

- **解决方法**

  一种基于多颗粒度特征融合的两阶段细粒度图像分类模型。该模型由两部分组成：多颗粒度特征融合模块和基于Vision-Transformer的两阶段分类。前者主要解决了单尺度特征的局限性。该模块提取骨干网络中不同尺度的特征，并将其融合，生成能够全面表示图像的特征。这些特征既包含高语义的全局信息，也包含低语义的局部信息。后者主要减少背景对预测的干扰。借助ViT模型，可以以非常小的成本实现物体的粗定位，从而使物体与背景分离。通过图像处理，对象占据了新图像中的主要成分，同时细节可以放大，更有利于最终的预测。

  - 具体贡献：
  - 1、我们提出了多颗粒度特征融合模块，以解决单尺度特征的局限性。该模块提取骨干网络中不同尺度的特征，并将其融合，生成能够综合表示图像的特征。
  - 2、我们提出了基于Vision-Transformer的两阶段分类，以减少背景对预测的干扰。借助ViT模型，可以将物体与背景分离，并将细节放大，更有利于最终的预测。
  - 3、广泛的实验和最佳的性能可以证明我们模型的优越性。可视化结果说明我们的两阶段分类可以准确定位物体，便于正确预测。

- **结果展示**

  ![image-20240825132152610](.\asset\1-s2.0-S0031320323007392-gr8_lrg.jpg)

​                                                 	<img src=".\asset\image-20240825132612133.png" alt="image-20240825132612133" style="zoom:50%;" />

- **总结**

  本文提出了一种基于Transform的多颗粒度特征融合的细粒度图像分类模型，该方法利用目前较为先进的Swin-Transformer模型提取特征，选择不同分辨率的特征图，通过多颗粒度特征融合模块，融合不同粒度的特征，并利用注意力机制对通道和空间两个维度的特征进行增强，融合后的特征既有高语义的全局信息，又有低语义的局部信息。此外，Vision-Transformer作为辅助模型，以极小的代价定位物体在图像中的位置。经过图像处理，最大程度地将物体与背景分离，从而减少对分类结果的影响。多颗粒度特征融合模块是一种即插即用模块，可以与目前流行的Transform和传统的CNN网络相结合。整个方法高效，具有很高的科研和应用价值。